{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## < DRAFT > Methods for Processing NFHP Habitat Condition and Disturbances to  Ecological and Jurisdictional  Units of Interest \n",
    "#### Daniel Wieferich -USGS\n",
    "\n",
    "This code summarizes NFHP Fish Habitat Condition Indicies to areas of ecological and jurisdictional units of interest (e.g. DOI lands, States, Ecoregions...), allowing us to understand fish habitat condition in units of management concern.  To create summaries first midpoints of each NFHP scored NHDPlusV1 flowline are assigned to spatial units that they fall within.  Habitat condition scores are summarized using a length-weighted average approach and disturbance variables for each of the four spatial scales are summarized by combining lists of disturbances from individual stream reaches into a common list of unique values.  These summaries are intended to be used in the National Biogeographic Map.\n",
    "\n",
    "This code is in progress and may change through time.\n",
    "\n",
    "#### Summaries to calculate per spatial unit\n",
    "    #1.length-weighted average hci per spatial unit\n",
    "    #2.total stream kilometers that were scored by NFHP within the spatial unit \n",
    "    #3.total stream kilometers that were not scored by NFHP within the spatial unit \n",
    "    #4.list of all significant anthropogenic disturbances effecting habitat condition within the spatial unit\n",
    "    #5.list of disturbances spatial unit for each spatial unit (lb, lc, nb, nc)\n",
    "    #6.stream kilometers within each disturbance class (e.g. very low risk to very high risk) within the spatial unit\n",
    "    \n",
    "    Below summaries still need to be addressed\n",
    "    #7.most pervasive disturbance effecting habitat condition within the spatial unit\n",
    "    #8.most severe disturbance effecting habitat condition within the spatial unit\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import needed packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bis2 import gc2\n",
    "import warnings\n",
    "import geojson\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creates table in database to insert resulting summaries \n",
    "\n",
    "q_createhci = \"CREATE TABLE IF NOT EXISTS nfhp.hci2015_summaries_mp ( \\\n",
    "    source_id varchar(250) primary key,\\\n",
    "    place_name varchar(250),\\\n",
    "    lc_hci2015_len_weight double precision,\\\n",
    "    nc_hci2015_len_weight double precision,\\\n",
    "    lb_hci2015_len_weight double precision,\\\n",
    "    nb_hci2015_len_weight double precision,\\\n",
    "    cumu_hci2015_len_weight double precision,\\\n",
    "    nc_list_dist text[],\\\n",
    "    lc_list_dist text[],\\\n",
    "    lb_list_dist text[],\\\n",
    "    nb_list_dist text[],\\\n",
    "    cumu_list_dist text[],\\\n",
    "    scored_km double precision,\\\n",
    "    not_scored_km double precision,\\\n",
    "    verylow_km double precision,\\\n",
    "    low_km double precision,\\\n",
    "    moderate_km double precision,\\\n",
    "    high_km double precision,\\\n",
    "    veryhigh_km double precision)\"\n",
    "url_createhci = gc2.sqlAPI(\"datadistillery\",\"bcb_beta\")+\"&q=\"+q_createhci\n",
    "print (requests.get(url_createhci,verify=False).json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defines Database requirements\n",
    "\n",
    "thisRun = {}\n",
    "thisRun['url'] = gc2.baseURLs[\"sqlapi_datadistillery_bcb_beta\"]\n",
    "thisRun['gc2Key'] = gc2.gc2Keys['datadistillery_bcb']  #Reads in api key, allowing for insert capabilities\n",
    "\n",
    "thisRun['fromSchema'] = 'sfr.'\n",
    "thisRun['fromTable'] = 'placenamelookup_poly'\n",
    "\n",
    "\n",
    "thisRun['insertSchema'] = 'nfhp.'\n",
    "thisRun['insertTable'] = 'hci2015_summaries_mp'\n",
    "thisRun['insertId'] = 'source_id'\n",
    "\n",
    "thisRun['fromSchemaTable'] = thisRun['fromSchema']+thisRun['fromTable']\n",
    "thisRun['insertSchemaTable'] = thisRun['insertSchema']+thisRun['insertTable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create csv to store disturbance variable information.  This is a temporary solution that is still being looked into.\n",
    "# This csv file currently reaches over 5GB which GC2 can not handle for upload.  Inserting row by row is SLOW so either the\n",
    "# data need to be processed concurrently with the length-weighted processes in this code or another data solution needs to be \n",
    "#explored. \n",
    "\n",
    "\n",
    "import csv\n",
    "fieldNames = 'source_id, comid, spatial_scale, disturbance  \\n'\n",
    "outFile = open('limiting_2015_mp_l.csv','a')\n",
    "outFile.write(fieldNames)\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#===============================================================================================================\n",
    "#===============================================================================================================\n",
    "# toProcessValidation identifies features that still need to be processed by comparing features in a from table\n",
    "# to the feature ids in the insert table (aka to table)\n",
    "#===============================================================================================================\n",
    "def toProcessValidation (thisRun):\n",
    "    #Retrieves a list of spatial units that need to be processed\n",
    "    toRun = []\n",
    "    ident = thisRun['insertId']\n",
    "    \n",
    "    queryUnits = thisRun['url']+\"?q=select \" + thisRun['fromSchemaTable'] + \".\" + ident + \" from \" +\\\n",
    "    thisRun['fromSchemaTable'] + \" left join \" + thisRun['insertSchemaTable'] + \" on \" +  \\\n",
    "    thisRun['insertSchemaTable'] + \".\" + ident + \" = \" + thisRun['fromSchemaTable']+ \".\" + ident +\\\n",
    "    \" where \" + thisRun['insertSchemaTable'] + \".\" + ident + \" is null\"\n",
    "    \n",
    "    featuresJson = requests.get(url=queryUnits,verify=False).json()\n",
    "    \n",
    "    if featuresJson != []:\n",
    "        toRunFeatures = featuresJson['features']\n",
    "        for feature in toRunFeatures:\n",
    "            toRun.append(feature['properties'][ident])\n",
    "    \n",
    "    return toRun\n",
    "\n",
    "#===============================================================================================================\n",
    "#===============================================================================================================\n",
    "#===============================================================================================================\n",
    "\n",
    "def requestHci(url, place):\n",
    "    import requests\n",
    "    import geopandas as gpd\n",
    "    import geojson\n",
    "\n",
    "    \n",
    "    queryHci = url+\"?q=select place_flow.comid, hci.lc_hci, hci.nc_hci, hci.lb_hci, hci.nb_hci, hci.cumu_hci, hci.lc_limit, hci.lb_limit, hci.nc_limit, hci.nb_limit, hci.cu_hcitext, place_flow.place_name, place_flow.lengthkm as seg_length from \\\n",
    "(select flow.comid, flow.lengthkm, place.place_name from nhd.nhdplusv1_midpoint_5070 flow inner join \\\n",
    "(select place_name, the_geom from sfr.placenamelookup_poly where source_id='\" + place + \"') place on ST_Within(flow.the_geom,place.the_geom) where flow.ftype in ('ArtificialPath','StreamRiver','Connector','CanalDitch')) place_flow \\\n",
    "left join nfhp.nfhp2015_hci hci on hci.comid=place_flow.comid\"\n",
    "    \n",
    "#    print (queryHci)\n",
    "    hciRequest = requests.get(url=queryHci, verify=False)\n",
    "    hciData = geojson.loads(hciRequest.text)\n",
    "    hciDf = gpd.GeoDataFrame.from_features(hciData['features'])\n",
    "    hciDf.crs ={'init':'epsg:5070'}\n",
    "    \n",
    "    return hciDf\n",
    "\n",
    "#===============================================================================================================\n",
    "#===============================================================================================================\n",
    "# hciSummaries calculates length weighted summaries of nfhp hci scores per feature polygon,\n",
    "# calculates lists of disturbances within the feature polygon and preps disturbance data to allow for \n",
    "# designations of pervasive and severe disturbances within the feature polygon (distValue dictionary)\n",
    "#===============================================================================================================\n",
    "\n",
    "def hciSummaries(hciDf):\n",
    "\n",
    "    #Summaries to calculate, followed by comment of definition\n",
    "    nc_limit = [] #unique list of upstream network catchment limiting disturbances\n",
    "    nb_limit = [] #unique list of upstream network buffer limiting disturbances\n",
    "    lc_limit = [] #unique list of local catchment limiting disturbances\n",
    "    lb_limit = [] #unique list of local buffer limiting disturbances\n",
    "    distValue = [] #list of dictionaries to populate disturbance per feature polygon table\n",
    "    notScored = 0  #Number of stream km not scored by NFHP within the feature polygon\n",
    "    scored = 0     #Number of stream km scored by NFHP within the feature polygon\n",
    "    weightCumu = 0 #length-weighted average summary of cumulative hci score\n",
    "    weightLc = 0   #length-weighted average summary of local catchment hci score\n",
    "    weightLb = 0   #length-weighted average summary of local buffer hci score\n",
    "    weightNc = 0   #length-weighted average summary of network catchment hci score\n",
    "    weightNb = 0   #length-weighted average summary of network buffer hci score\n",
    "    veryLow = 0\n",
    "    low = 0\n",
    "    mod = 0\n",
    "    high = 0\n",
    "    veryHigh = 0\n",
    "\n",
    "    \n",
    "    for row in hciDf.itertuples():\n",
    "\n",
    "\n",
    "        #Calculates total stream km with and without habitat condition score within the feature polygon\n",
    "        #only run on rows that have habitat condition index values, this allows us to capture scored vs not scored kms\n",
    "\n",
    "        if row.cumu_hci:\n",
    "\n",
    "            if row.cu_hcitext:\n",
    "                if row.cu_hcitext == 'Very low':\n",
    "                    veryLow += float(row.seg_length)\n",
    "                elif row.cu_hcitext == 'Low':\n",
    "                    low += float(row.seg_length)\n",
    "                elif row.cu_hcitext == 'Moderate':\n",
    "                    mod += float(row.seg_length)\n",
    "                elif row.cu_hcitext == 'High':\n",
    "                    high += float(row.seg_length)\n",
    "                elif row.cu_hcitext == 'Very high':\n",
    "                    veryHigh += float(row.seg_length)\n",
    "\n",
    "                        \n",
    "            scored += float(row.seg_length)\n",
    "            if row.cumu_hci:\n",
    "                weightCumu += (float(row.seg_length))*(float(row.cumu_hci))\n",
    "            if row.lc_hci:\n",
    "                weightLc += (float(row.seg_length))*(float(row.lc_hci))\n",
    "            if row.lb_hci:\n",
    "                weightLb += (float(row.seg_length))*(float(row.lb_hci))\n",
    "            if row.nc_hci:\n",
    "                weightNc += (float(row.seg_length))*(float(row.nc_hci))\n",
    "            if row.nb_hci:\n",
    "                weightNb += (float(row.seg_length))*(float(row.nb_hci))\n",
    "            \n",
    "            #Calculating List Disturbances, Most Pervasive and Most Severe\n",
    "            if row.nc_limit:\n",
    "                for word in row.nc_limit.split(';'):\n",
    "                    nc_limit.append(word)\n",
    "                    distValue.append({'comid':int(row.comid), 'spatial_scale':'nc', 'disturbance':word})\n",
    "            if row.nb_limit:\n",
    "                for word in row.nb_limit.split(';'):\n",
    "                    nb_limit.append(word)\n",
    "                    distValue.append({'comid':int(row.comid), 'spatial_scale':'nb', 'disturbance':word})\n",
    "            if row.lc_limit:\n",
    "                for word in row.lc_limit.split(';'):\n",
    "                    lc_limit.append(word)\n",
    "                    distValue.append({'comid':int(row.comid), 'spatial_scale':'lc', 'disturbance':word})\n",
    "            if row.lb_limit:\n",
    "                for word in row.lb_limit.split(';'):\n",
    "                    lb_limit.append(word)\n",
    "                    distValue.append({'comid':int(row.comid), 'spatial_scale':'lb', 'disturbance':word})\n",
    "        \n",
    "        #for unscored streams track cumulative unscored stream km\n",
    "        else:\n",
    "            notScored += float(row.seg_length)\n",
    "            \n",
    "\n",
    "    summaryValues = {'nc_list_dist':list(set(nc_limit)),'nb_list_dist':list(set(nb_limit)),\n",
    "                     'lc_list_dist':list(set(lc_limit)),'lb_list_dist':list(set(lb_limit)),\n",
    "                     'cumu_list_dist':list(set(lb_limit+lc_limit+nb_limit+nc_limit)),\n",
    "                    'lc_hci2015_len_weight':weightLc/scored, 'nc_hci2015_len_weight': weightNc/scored,\n",
    "                    'lb_hci2015_len_weight':weightLb/scored, 'nb_hci2015_len_weight': weightNb/scored,\n",
    "                    'cumu_hci2015_len_weight':weightCumu/scored, \n",
    "                    'not_scored_km':notScored, 'scored_km':scored, 'verylow_km': veryLow, 'low_km': low, 'moderate_km': mod,\n",
    "                    'high_km': high, 'veryhigh_km': veryHigh}\n",
    "    \n",
    "    return distValue, summaryValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Identify and open csv file to dump disturbance data into\n",
    "outFileName = 'limiting_2015_mp_l.csv'\n",
    "outFile = open(outFileName,'a')\n",
    "\n",
    "\n",
    "#Capture spatial units where code fails\n",
    "failLog = []\n",
    "\n",
    "#Determines which spatial units still need to be summarized\n",
    "toRun = toProcessValidation (thisRun)\n",
    "while toRun != []:\n",
    "    \n",
    "    for run in toRun:\n",
    "        try:\n",
    "            #Run function to request data for a given spatial unit\n",
    "            hciDf = requestHci(thisRun['url'], run)\n",
    "            # Summarize data for a given spatial unit, return results\n",
    "            distValue, summaryValues = hciSummaries(hciDf)\n",
    "\n",
    "\n",
    "            #Query for inserting resulting summaries into BIS Database \n",
    "            summaryInsert = \"insert into nfhp.hci2015_summaries_mp(source_id, place_name, lc_hci2015_len_weight, nc_hci2015_len_weight, \\\n",
    "lb_hci2015_len_weight, nb_hci2015_len_weight, cumu_hci2015_len_weight, nc_list_dist, lc_list_dist, lb_list_dist, \\\n",
    "nb_list_dist, cumu_list_dist, scored_km, not_scored_km, verylow_km, low_km, moderate_km, high_km, veryhigh_km) VALUES ('\" + str(run) + \"' ,'\"  + str(hciDf['place_name'][0]) + \"' ,'\" \\\n",
    "+ str(summaryValues['lc_hci2015_len_weight']) + \"' ,'\" + str(summaryValues['nc_hci2015_len_weight']) + \"' ,'\" + \\\n",
    "str(summaryValues['lb_hci2015_len_weight'])+ \"' ,'\" + str(summaryValues['nb_hci2015_len_weight'])+ \"' ,'\" + \\\n",
    "str(summaryValues['cumu_hci2015_len_weight'])+ \"' , ARRAY\" + str(summaryValues['nc_list_dist'])+ \"  , ARRAY\" + \\\n",
    "str(summaryValues['lc_list_dist'])+ \", ARRAY\" + str(summaryValues['lb_list_dist'])+ \" , ARRAY\" + \\\n",
    "str(summaryValues['nb_list_dist']) + \", ARRAY\" + str(summaryValues['cumu_list_dist'])+ \",'\" \\\n",
    "+ str(summaryValues['scored_km'])+ \"' ,'\" + str(summaryValues['not_scored_km'])+ \"' ,'\" + str(summaryValues['verylow_km']) \\\n",
    "+ \"' ,'\" + str(summaryValues['low_km'])+ \"' ,'\" + str(summaryValues['moderate_km']) \\\n",
    "+ \"' ,'\" + str(summaryValues['high_km'])+ \"' ,'\" + str(summaryValues['veryhigh_km'])+\"')\"\n",
    "    \n",
    "            #Code that inserts data\n",
    "            payload = \"?q=%s&key=%s\"%(summaryInsert,thisRun['gc2Key'])\n",
    "            queryUrl = thisRun['url']+payload\n",
    "            r = requests.get(queryUrl, verify=False)\n",
    "            \n",
    "            \n",
    "            #write rows of disturbance values into the csv (the thought was to add the csv to BIS database through UI upon completion)\n",
    "            for record in distValue:\n",
    "                newRow = (str(run) + \", \" + str(record['comid']) + \", \" + str(record['spatial_scale']) + \" ,\" +str(record['disturbance'])+ '\\n')\n",
    "                outFile.write(newRow)\n",
    "            \n",
    "    \n",
    "            \n",
    "        except:\n",
    "            #print (run + ' did not work')\n",
    "            failLog.append(str(run))\n",
    "            continue\n",
    "            \n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The code below runs summaries on one spatial feature (e.g. lcc, state, ecoregion...) from the placenamelookup table. \n",
    "#This code was used for testing and also times the process.\n",
    "\n",
    "import time\n",
    "run = 'gu_state:e57dfa3f-244d-4dbe-9d45-c034dbda861a'\n",
    "start_time = time.clock()\n",
    "try:\n",
    "    hciDf = requestHci(thisRun['url'], run)\n",
    "    time1 = time.clock()\n",
    "    print (str((time1 - start_time)/60) + ' minutes first process')\n",
    "    distValue, summaryValues = hciSummaries(hciDf)\n",
    "    time2 = time.clock()\n",
    "    print (str((time2 - time1)/60) + ' minutes 2nd process')\n",
    "    \n",
    "#    result = mongoDist.insert_many(distValue)\n",
    "    \n",
    "    \n",
    "    summaryInsert = \"insert into nfhp.hci2015_summaries_mp(source_id, place_name, lc_hci2015_len_weight, nc_hci2015_len_weight, \\\n",
    "lb_hci2015_len_weight, nb_hci2015_len_weight, cumu_hci2015_len_weight, nc_list_dist, lc_list_dist, lb_list_dist, \\\n",
    "nb_list_dist, cumu_list_dist, scored_km, not_scored_km, verylow_km, low_km, moderate_km, high_km, veryhigh_km) VALUES ('\" + str(run) + \"' ,'\"  + str(hciDf['place_name'][0]) + \"' ,'\" \\\n",
    "+ str(summaryValues['lc_hci2015_len_weight']) + \"' ,'\" + str(summaryValues['nc_hci2015_len_weight']) + \"' ,'\" + \\\n",
    "str(summaryValues['lb_hci2015_len_weight'])+ \"' ,'\" + str(summaryValues['nb_hci2015_len_weight'])+ \"' ,'\" + \\\n",
    "str(summaryValues['cumu_hci2015_len_weight'])+ \"' , ARRAY\" + str(summaryValues['nc_list_dist'])+ \"  , ARRAY\" + \\\n",
    "str(summaryValues['lc_list_dist'])+ \", ARRAY\" + str(summaryValues['lb_list_dist'])+ \" , ARRAY\" + \\\n",
    "str(summaryValues['nb_list_dist']) + \", ARRAY\" + str(summaryValues['cumu_list_dist'])+ \",'\" \\\n",
    "+ str(summaryValues['scored_km'])+ \"' ,'\" + str(summaryValues['not_scored_km'])+ \"' ,'\" + str(summaryValues['verylow_km']) \\\n",
    "+ \"' ,'\" + str(summaryValues['low_km'])+ \"' ,'\" + str(summaryValues['moderate_km']) \\\n",
    "+ \"' ,'\" + str(summaryValues['high_km'])+ \"' ,'\" + str(summaryValues['veryhigh_km'])+\"')\"\n",
    "    \n",
    "    payload = \"?q=%s&key=%s\"%(summaryInsert,thisRun['gc2Key'])\n",
    "    queryUrl = thisRun['url']+payload\n",
    "    r = requests.get(queryUrl, verify=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    outFileName = 'limiting_2015_mp_l.csv'\n",
    "    outFile = open(outFileName,'a')\n",
    "    for record in distValue:\n",
    "        newRow = (str(run) + \", \" + str(record['comid']) + \", \" + str(record['spatial_scale']) + \" ,\" +str(record['disturbance'])+ '\\n')\n",
    "        #print (newRow)\n",
    "        outFile.write(newRow)\n",
    "    outFile.close()\n",
    "    \n",
    "except:\n",
    "    print (run + ' did not work')\n",
    "\n",
    "\n",
    "end_time = time.clock()\n",
    "print (str((end_time-start_time)/60) + \" minutes total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################BELOW IS TESTING TO SUMMARIZE DISTURBANCE INFORMATON: STILL IN PROGRESS################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Group by to summarize cumulative (not specific to scale or stream size) most severe disturbances\n",
    "listDf = pd.DataFrame(distValue)\n",
    "listDf.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "severeAll = listDf.loc[listDf['cu_hcitext'].isin(['Very high','High'])]\n",
    "severeAll.segLen = severeAll.segLen.apply(float)\n",
    "groupDf = severeAll.groupby(['comid','disturbance']).mean()\n",
    "groupDf.reset_index(inplace=True)\n",
    "sumGroupDf = groupDf.groupby(['disturbance']).sum()\n",
    "sumGroupDf.reset_index(inplace=True)\n",
    "del sumGroupDf['comid']\n",
    "#sumGroupDf\n",
    "\n",
    "sumGroupDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumGroupDf.groupby(['disturbance'], sort=False)['segLen'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumGroupDf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
